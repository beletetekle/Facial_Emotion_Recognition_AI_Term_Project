{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.models import model_from_json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "\n",
    "The Face data we used for training is taken from kaggle in Representation Learning: Facial Expression Recognition Challenge from 2013. the data consists of 48x48 pixel grayscale images of faces arranged in a row and seven categories/labels (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral) of emotions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "def xyFromDataset(xT,yT):\n",
    "    \n",
    "    #directories of images for each 7 emotions\n",
    "    category = [cat for cat in os.listdir('training_sett')]\n",
    "    \n",
    "    #iterate through each category to extract image data set for each emotion\n",
    "    for catT in category:\n",
    "        current_image_path = os.path.join('training_sett', catT)\n",
    "        imageList = [i for i in os.listdir(current_image_path) if not i.startswith('.')]\n",
    "        \n",
    "        #read image data as piexels and prepare training variables x,y \n",
    "        for i in imageList:\n",
    "            imageT = cv2.imread(os.path.join(current_image_path, i))                \n",
    "            imageT = cv2.cvtColor(imageT, cv2.COLOR_BGR2GRAY)\n",
    "            tI=cv2.resize(imageT,(48,48))\n",
    "            trainPixels = image.img_to_array(tI)\n",
    "            trainPixels = np.expand_dims(trainPixels, axis = 0)\n",
    "            trainPixels /= 255  \n",
    "            xT.append(np.array(trainPixels,\"float32\"))\n",
    "            yT.append(float(catT))\n",
    "    #finalizing\n",
    "    xT = np.array(xT)\n",
    "    yT = np.array(yT,'float32')\n",
    "    yT = np_utils.to_categorical(yT, num_classes=7)\n",
    "\n",
    "    xT -= np.mean(xT, axis=0)\n",
    "    xT /= np.std(xT, axis=0)\n",
    "    \n",
    "    xT = xT.reshape(xT.shape[0], 48, 48, 1)\n",
    "    \n",
    "    offset = int(xT.shape[0]-xT.shape[0]/4)\n",
    "    offset2 = int(xT.shape[0]-xT.shape[0]/6)\n",
    "\n",
    "    xTT = xT[0:offset]\n",
    "    yTT = yT[0:offset]\n",
    "    xV = xT[offset: offset +  offset2]\n",
    "    yV = yT[offset: offset + offset2]\n",
    "    xTest = xT[offset +  offset2:]\n",
    "    yTest = yT[offset +  offset2:]\n",
    "    \n",
    "    return xTT, yTT,xV,yV,xTest,yTest\n",
    "\n",
    "#capture training data\n",
    "def gatherData(source,emotions):\n",
    "    \n",
    "    #local variables for file name and directory name\n",
    "    i=0\n",
    "    label = 0\n",
    "    m = \"/record\"\n",
    "    a = str(time.time())\n",
    "    \n",
    "    #current emotion data save location\n",
    "    categoryPath = os.path.join('training_sett', str(label))\n",
    "    \n",
    "    #start capturing data using opencv and load face detection dataset 'frontalface_default.xml'\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    #track key press\n",
    "    K = None\n",
    "\n",
    "    while True:\n",
    "\n",
    "        #capture frame from source\n",
    "        val, img = liveFrame.read()\n",
    "        \n",
    "        #if no frame is captured continue\n",
    "        if not val:\n",
    "            continue;\n",
    "        \n",
    "        #convert captured image into grayscale for management ease\n",
    "        imageT = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        #invoke face detection on the captured frame\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(imageT, 1.32, 5)\n",
    "        \n",
    "        #loop through detected face list and save each face to the corresponding emotion directory label\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=4)\n",
    "            cv2.putText(img, emotions[label] + m, (int(x), int(y)), cv2.FONT_HERSHEY_DUPLEX, 1, (120,0,255), 2)\n",
    "            \n",
    "            if K == ord('w') or m == \"/recording\":    \n",
    "                \n",
    "                m = \"/recording\"\n",
    "                cp = os.path.join(categoryPath, a + str(i) + \".jpg\")\n",
    "                predImg=imageT[y:y+w,x:x+h]\n",
    "                predImg=cv2.resize(predImg,(48,48))\n",
    "                cv2.imwrite(cp, predImg)\n",
    "                i+=1\n",
    "                print (cp)\n",
    "                \n",
    "            if K == ord('q') and m == \"/recording\":\n",
    "                \n",
    "                m = \"/record\"\n",
    "                if(label < 6):\n",
    "                    label+=1\n",
    "                    categoryPath = os.path.join('training_sett', str(label))\n",
    "                else:\n",
    "                    break \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "        #open openCV's display window with specified frame size\n",
    "        resized_img = cv2.resize(img, (1000, 700))\n",
    "        cv2.imshow('Collecting Data',resized_img)\n",
    "        K = cv2.waitKey(10) & 0xFF\n",
    "        \n",
    "        #close openCV's window if key 'e' is pressed\n",
    "        if K == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "    \n",
    "    \n",
    "#user fer 2013 instead\n",
    "def xyFromDatasetFer2013():\n",
    "    #prepare training variables using fer2013 dataset\n",
    "    #load the face data using panda from csv and extract triaining data from it.\n",
    "    faceData = pd.read_csv('fer2013.csv').iterrows()\n",
    "\n",
    "    trainY = []\n",
    "    trainX = []\n",
    "\n",
    "    textY = []\n",
    "    testX = []\n",
    "\n",
    "    for i,j in faceData:\n",
    "\n",
    "        if 'Training' in j['Usage']:\n",
    "            trainX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "            trainY.append(j['emotion'])\n",
    "\n",
    "        elif 'PublicTest' in j['Usage']:\n",
    "            testX.append(np.array(j['pixels'].split(\" \"),'float32'))\n",
    "            textY.append(j['emotion'])\n",
    "    #flatten the data array and normalize between 1 and 0 using mean and standard deviation for x\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    trainY = np_utils.to_categorical(trainY, num_classes=7)\n",
    "\n",
    "    trainX -= np.mean(trainX, axis=0)\n",
    "    trainX /= np.std(trainX, axis=0)\n",
    "\n",
    "    testX = np.array(testX)\n",
    "    textY = np.array(textY)\n",
    "    textY = np_utils.to_categorical(textY, num_classes=7)\n",
    "\n",
    "    testX -= np.mean(testX, axis=0)\n",
    "    testX /= np.std(testX, axis=0)\n",
    "\n",
    "\n",
    "    #reshape the matrices to represent each face data\n",
    "    trainX = trainX.reshape(trainX.shape[0], 48, 48, 1)\n",
    "    testX = testX.reshape(testX.shape[0], 48, 48, 1)\n",
    "    \n",
    "    return trainX,trainY,testX,textY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compie and fit the CNN model\n",
    "def comFit(model,trainX, trainY,epoch,bs,testX=None, testY=None):\n",
    "    #Compliling the model\n",
    "    model.compile(loss=categorical_crossentropy,optimizer=Adam(),metrics=['accuracy'])\n",
    "\n",
    "    #Training the model\n",
    "    model.fit(trainX, trainY,epochs=epoch, batch_size=bs,verbose=1,shuffle=True, validation_data=(testX, testY),)\n",
    "    \n",
    "\n",
    "    \n",
    "    return model\n",
    "\n",
    "#real time emotion prediction from specified source\n",
    "def realPred(source,model,emotions):\n",
    "    liveFrame = cv2.VideoCapture(source)\n",
    "    faceDetectioin = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "\n",
    "        val, img = liveFrame.read()\n",
    "\n",
    "        if not val:\n",
    "            continue;\n",
    "\n",
    "        testImg = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        detectedFaces = faceDetectioin.detectMultiScale(testImg, 1.32, 5)\n",
    "\n",
    "        for (x,y,w,h) in detectedFaces:\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,128,0),thickness=4)\n",
    "\n",
    "            predImg=testImg[y:y+w,x:x+h]\n",
    "            predImg=cv2.resize(predImg,(48,48))\n",
    "            predPixels = image.img_to_array(predImg)\n",
    "            predPixels = np.expand_dims(predPixels, axis = 0)\n",
    "            predPixels /= 255\n",
    "\n",
    "            pridectedList = model.predict(predPixels)\n",
    "           \n",
    "            maxVal = np.argmax(pridectedList[0])\n",
    "            predEmotion = emotions[maxVal]\n",
    "            \n",
    "            path = os.path.join(\"emot\",predEmotion+\".png\")\n",
    "          \n",
    "            \n",
    "            emoji = cv2.imread(path,-1)\n",
    "            emoji = cv2.resize(emoji,(50,50))\n",
    "            \n",
    "            y1, y2 = int(y), int(y) + emoji.shape[0]\n",
    "            x1, x2 = int(x), int(x) + emoji.shape[1]\n",
    "            \n",
    "            ealpha = emoji[:, :, 3] / 255.0\n",
    "            ialpha = 1.0 - ealpha\n",
    "\n",
    "            for c in range(0, 3):\n",
    "                img[y1:y2, x1:x2, c] = (ealpha * emoji[:, :, c] + ialpha * img[y1:y2, x1:x2, c])\n",
    "            \n",
    "            cv2.putText(img, predEmotion, (int(x), int(y)), cv2.FONT_HERSHEY_DUPLEX, 1, (120,0,255), 2)\n",
    "\n",
    "        resized_img = cv2.resize(img, (1000, 600))\n",
    "        cv2.imshow('Analayzing Facial Emotion',resized_img)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('e'):\n",
    "            break\n",
    "\n",
    "    liveFrame.release()\n",
    "    cv2.destroyAllWindows\n",
    "    #exit()\n",
    "#CNN Model from training set\n",
    "def craeateModel(trainX,labels):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(trainX.shape[1:])))\n",
    "    model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(labels, activation='softmax'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#possible target emotions\n",
    "emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "\n",
    "#training and testing data xy\n",
    "xT,yT,xV,yV = [],[],[],[]\n",
    "\n",
    "val = xyFromDataset(xT,yT)\n",
    "\n",
    "xT = np.asarray(val[0])\n",
    "yT = np.asarray(val[1])\n",
    "xV = np.asarray(val[2])\n",
    "yV = np.asarray(val[3])\n",
    "xTest = np.asarray(val[4])\n",
    "yTest = np.asarray(val[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature filtering\n",
    "We are experimenting with different layers of Convolutional Neural Network (CNN) for training our model and For feature extraction. At the time of our project release we will decide on the optimal settings and layers for the CNN. We will apply max-pooling to reduce the dimensions of the data, currently we are using a pool size of 2, with strides 2 by 2. Finally we will flatten the image matrix and pass it through a fully connected layer to classify the images.\n",
    "Since we only have one input (image pixels) and one output (the predicted emotion, 0 to 6), We used sequential class from keras for creating the CNN model. We add each layer instance with conv2d method, which creates convolution layer and the kernel size we are going to use is yet to be decided based on the final list of our dataset, currently we are using kernel size 4 by 4.  Each layer is convolved with the layer input to produce a tensor of outputs. The activation function used is relu, which activate each neuron (not all at once) to produce output for the next layer. The dropout rate to use is yet to be decided as we modify the layer settings. MaxPolling2D is used with window size and stride of 2 in current settings in each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = craeateModel(xT,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Compile and train the model\n",
    "\n",
    "in the model configuration, with use adam optimizer and 'accuracy' as a metric to evaluate the model during the training and testing. And a loss/objective function used is crossentropy.\n",
    "\n",
    "in the triaining of the model, number of samples per gradient update is set to be 64, max iteration is set to be 32, verbosity mode used is progress bar during training and shuffle is on , to shuffle the training data before each epoch. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "72/72 [==============================] - 62s 824ms/step - loss: 1.7888 - accuracy: 0.1991 - val_loss: 6.6534 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      "72/72 [==============================] - 59s 814ms/step - loss: 1.4377 - accuracy: 0.4192 - val_loss: 10.1172 - val_accuracy: 0.0794\n",
      "Epoch 3/20\n",
      "72/72 [==============================] - 59s 822ms/step - loss: 0.6513 - accuracy: 0.7648 - val_loss: 11.7274 - val_accuracy: 0.0374\n",
      "Epoch 4/20\n",
      "72/72 [==============================] - 64s 885ms/step - loss: 0.3483 - accuracy: 0.8811 - val_loss: 12.1282 - val_accuracy: 0.0203\n",
      "Epoch 5/20\n",
      "72/72 [==============================] - 66s 921ms/step - loss: 0.2101 - accuracy: 0.9309 - val_loss: 14.0279 - val_accuracy: 0.0368\n",
      "Epoch 6/20\n",
      "72/72 [==============================] - 62s 856ms/step - loss: 0.1701 - accuracy: 0.9432 - val_loss: 12.7955 - val_accuracy: 0.0587\n",
      "Epoch 7/20\n",
      "72/72 [==============================] - 61s 850ms/step - loss: 0.1478 - accuracy: 0.9517 - val_loss: 12.7083 - val_accuracy: 0.0469\n",
      "Epoch 8/20\n",
      "72/72 [==============================] - 59s 827ms/step - loss: 0.1400 - accuracy: 0.9527 - val_loss: 13.7984 - val_accuracy: 0.0614\n",
      "Epoch 9/20\n",
      "72/72 [==============================] - 59s 818ms/step - loss: 0.0955 - accuracy: 0.9667 - val_loss: 13.4354 - val_accuracy: 0.0410\n",
      "Epoch 10/20\n",
      "72/72 [==============================] - 58s 810ms/step - loss: 0.0828 - accuracy: 0.9727 - val_loss: 16.8547 - val_accuracy: 0.0381\n",
      "Epoch 11/20\n",
      "72/72 [==============================] - 59s 819ms/step - loss: 0.0961 - accuracy: 0.9675 - val_loss: 13.5227 - val_accuracy: 0.0528\n",
      "Epoch 12/20\n",
      "72/72 [==============================] - 58s 810ms/step - loss: 0.0667 - accuracy: 0.9777 - val_loss: 14.8909 - val_accuracy: 0.0532\n",
      "Epoch 13/20\n",
      "72/72 [==============================] - 60s 833ms/step - loss: 0.0844 - accuracy: 0.9720 - val_loss: 15.7199 - val_accuracy: 0.0505\n",
      "Epoch 14/20\n",
      "72/72 [==============================] - 59s 816ms/step - loss: 0.0771 - accuracy: 0.9757 - val_loss: 14.9654 - val_accuracy: 0.0699\n",
      "Epoch 15/20\n",
      "72/72 [==============================] - 59s 823ms/step - loss: 0.0737 - accuracy: 0.9760 - val_loss: 13.6132 - val_accuracy: 0.0745\n",
      "Epoch 16/20\n",
      "72/72 [==============================] - 59s 822ms/step - loss: 0.0598 - accuracy: 0.9780 - val_loss: 15.9238 - val_accuracy: 0.0312\n",
      "Epoch 17/20\n",
      "72/72 [==============================] - 60s 828ms/step - loss: 0.0621 - accuracy: 0.9802 - val_loss: 18.3049 - val_accuracy: 0.0368\n",
      "Epoch 18/20\n",
      "72/72 [==============================] - 59s 817ms/step - loss: 0.0532 - accuracy: 0.9824 - val_loss: 15.2068 - val_accuracy: 0.0505\n",
      "Epoch 19/20\n",
      "72/72 [==============================] - 59s 826ms/step - loss: 0.0581 - accuracy: 0.9816 - val_loss: 17.1110 - val_accuracy: 0.0433\n",
      "Epoch 20/20\n",
      "72/72 [==============================] - 59s 821ms/step - loss: 0.0508 - accuracy: 0.9846 - val_loss: 15.5139 - val_accuracy: 0.0417\n"
     ]
    }
   ],
   "source": [
    "model =  comFit(model,xT, yT,20,128,xV,yV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Facial Emotion Prediction\n",
    "\n",
    "capturing live frames:\n",
    "we used pc camera as source of video and we used openCV library.we used VideoCapture class with '0' as an argument for capturing videos from the default source of video(camera). We created an infinite loop to capture frames  from the vieo iteratively using the read method of VideoCapture class and convert the frame into grayscale.\n",
    "\n",
    "detecting facial region:\n",
    "We again make use of openCV's face detection api CascadeClassifier, to detect facial arias of the captured frame. The training data set we use for CascadeClassifier is haaarcascade_frontalface_default.xml. detectMultiScale returns the detected faces as list of rectangles and their locations.\n",
    "\n",
    "prediction:\n",
    "We loop through the detected face recangles and draw the rectangles on the screen. we select area of interst from the image based on the returned rectangle and the location of the image. convert the selected area into array of metrices containing pixles after resizing the image based on our training data. reduce the rgb representation by dividing it by 255 and give the image to our model for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving our model for later prediction use\n",
    "modelJson = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(modelJson)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load our model for prediction\n",
    "'''model = model_from_json(open(\"model3.json\", \"r\").read())\n",
    "model.load_weights('model3.h5')\n",
    "'''\n",
    "realPred(0,model,emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluation of our model\n",
    "model.evaluate(xTest, yTest, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
